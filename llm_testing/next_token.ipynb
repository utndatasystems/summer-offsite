{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/rengert/summer-offsite/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing ipywidgets...\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Installing ipywidgets...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\".cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\".cache\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_token(prompt, k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    topk_values, topk_indices = torch.topk(logits, top_k, dim=-1)\n",
    "    top_tokens = tokenizer.batch_decode(topk_indices[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    # print(topk_indices[0][:k])\n",
    "    \n",
    "    return top_tokens, topk_values[0]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_token_prop(prompt, k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    print(tokenizer.batch_decode(\n",
    "        inputs['input_ids'][0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    print(outputs)\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n",
    "\n",
    "    # Apply softmax to top-k logits to get probabilities\n",
    "    topk_probs = F.softmax(topk_values, dim=-1)\n",
    "\n",
    "    top_tokens = tokenizer.batch_decode(\n",
    "        topk_indices[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    return top_tokens, topk_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[4340,  646,  498]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "['How', ' can', ' you']\n",
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 8.2367,  6.6111,  4.5392,  ..., -4.0680, -4.0680, -4.0681],\n",
      "         [ 4.1064,  5.5320,  4.0867,  ..., -3.4824, -3.4814, -3.4826],\n",
      "         [ 6.9169,  8.1698,  9.0604,  ..., -3.9949, -3.9945, -3.9951]]]), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7a6a61b6f440>, hidden_states=None, attentions=None)\n",
      "1.  determine (logit: 0.2247)\n",
      "2.  use (logit: 0.1334)\n",
      "3.  find (logit: 0.1147)\n",
      "4.  modify (logit: 0.1048)\n",
      "5.  simplify (logit: 0.0876)\n",
      "6.  calculate (logit: 0.0757)\n",
      "7.  solve (logit: 0.0678)\n",
      "8.  express (logit: 0.0650)\n",
      "9.  prove (logit: 0.0634)\n",
      "10.  create (logit: 0.0630)\n",
      "Sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How can you\"\n",
    "top_k = 10\n",
    "\n",
    "top_tokens, top_values = top_k_token_prop(prompt, k=top_k)\n",
    "for i, (token, score) in enumerate(zip(top_tokens, top_values)):\n",
    "    print(f\"{i+1}. {token} (logit: {score.item():.4f})\")\n",
    "print(f\"Sum: {top_values[0:10].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice| was| beginning| to| get| very| tired| of| sitting| by| her| sister| on| the| bank|.\n",
      "13\n",
      "Alice was beginning to get very tired of sitting by her sister on the| was beginning to get very tired of sitting by her sister on the bank| beginning to get very tired of sitting by her sister on the bank.\n",
      "tensor([[61686,   572,  7167,   311,   633,  1602, 19227,   315, 11699,   553,\n",
      "          1059, 12923,   389,   279],\n",
      "        [  572,  7167,   311,   633,  1602, 19227,   315, 11699,   553,  1059,\n",
      "         12923,   389,   279,  6073],\n",
      "        [ 7167,   311,   633,  1602, 19227,   315, 11699,   553,  1059, 12923,\n",
      "           389,   279,  6073,    13]])\n",
      " the| bank|.\n",
      "0.9598|0.0177|0.2120\n",
      "Encoder: Completed 0 %\n",
      "Compression Ratio for Arithmetic Coding :  0.7 bits/char\n",
      "{'characters': 10, 'tokens': 3, 'entropy': ['0.8113'], 'Llama+AC compressed file size': 7, 'bits per character': 0.7}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from arithmetic_coder import ArithmeticDecoder, ArithmeticEncoder, BitInputStream, BitOutputStream\n",
    "\n",
    "\n",
    "def gen_rank(probs, next_token):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True, stable=True)\n",
    "    rank_list = []\n",
    "    if next_token.shape[0] > 1:\n",
    "        for i in range(next_token.shape[0]):\n",
    "            rank_list += [torch.where(probs_idx[i:i+1, :] == next_token[i])[-1]]\n",
    "        rank = torch.squeeze(torch.stack(rank_list))\n",
    "    else:\n",
    "        rank = torch.where(probs_idx == next_token)[-1]\n",
    "    return rank\n",
    "\n",
    "\n",
    "def read_bitstream(bitin):\n",
    "    temp_list = []\n",
    "    while True:\n",
    "        temp = bitin.read()\n",
    "        if temp == -1:\n",
    "            break\n",
    "        temp_list += [temp]\n",
    "    temp_arr = np.array(temp_list)\n",
    "    final_ind = (np.where(temp_arr == 1)[0][-1]).astype(int)\n",
    "    final_arr = temp_arr[:final_ind+1]\n",
    "\n",
    "    return final_arr\n",
    "\n",
    "\n",
    "def print_tokens(tokens):\n",
    "    decoded_tokens = tokenizer.batch_decode(\n",
    "        tokens[0:1000],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    print(\"|\".join(decoded_tokens))\n",
    "\n",
    "\n",
    "def print_probs(probs):\n",
    "    print(\"|\".join([f\"{p:.4f}\" for p in probs]))\n",
    "\n",
    "\n",
    "class LLMzip_encode:\n",
    "    def __init__(self, model, tokenizer, filename):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.filename = filename\n",
    "        self.file_out = open(self.filename+'_llmzip_ac.txt', 'wb')\n",
    "        self.bitout = BitOutputStream(self.file_out)\n",
    "        self.AC_encoder = ArithmeticEncoder(32, self.bitout)\n",
    "\n",
    "        self.AC_encoder = ArithmeticEncoder(32, self.bitout)\n",
    "\n",
    "        self.alphabet_size = self.model.config.vocab_size\n",
    "\n",
    "        self.token_length = 0\n",
    "        self.starter_tokens = []\n",
    "\n",
    "    def encode_batch(self, prompt_tokens):\n",
    "        print_tokens(prompt_tokens)\n",
    "        bsz = prompt_tokens.shape[0]\n",
    "\n",
    "        prompt_size = prompt_tokens.shape[1]\n",
    "\n",
    "        tokens = torch.full((bsz, prompt_size), self.tokenizer.pad_token_id).long()\n",
    "        tokens[:bsz, : prompt_size] = torch.tensor(prompt_tokens).long()\n",
    "        print(tokens)\n",
    "\n",
    "        cur_pos = prompt_size-1\n",
    "        prev_pos = 0\n",
    "\n",
    "        logits = self.model.forward(tokens[:, prev_pos:cur_pos]).logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        rank = gen_rank(probs, next_token=tokens[:, cur_pos])\n",
    "\n",
    "        probs_np2 = probs.cpu().detach().numpy()\n",
    "        tokens_np2 = tokens[:, cur_pos].cpu().numpy()\n",
    "        ranks_np2 = rank.cpu().numpy()\n",
    "\n",
    "        probs_tok = probs_np2[np.arange(bsz), tokens_np2]\n",
    "        print_tokens(tokens_np2)\n",
    "        print_probs(probs_tok)\n",
    "\n",
    "        cumul = np.zeros(self.model.vocab_size+1, dtype=np.uint64)\n",
    "        for j in range(bsz):\n",
    "            prob1 = probs_np2[j]\n",
    "            cumul[1:] = np.cumsum(prob1*10000000 + 1)\n",
    "            self.AC_encoder.write(cumul, tokens_np2[j])\n",
    "\n",
    "        return ranks_np2, probs_tok\n",
    "\n",
    "    def encode(self, win_size: int):\n",
    "        if not os.path.exists(self.filename + '_tokens.npy'):\n",
    "            with open(self.filename, 'r') as f_in:\n",
    "                text_input = f_in.read()\n",
    "\n",
    "            tokens_full = np.array(tokenizer.encode(text_input))\n",
    "            np.save(self.filename + '_tokens.npy', tokens_full)\n",
    "        else:\n",
    "            tokens_full = np.load(self.filename + '_tokens.npy')\n",
    "\n",
    "        tokens_full = tokens_full[0:1000]\n",
    "        print_tokens(tokens_full)\n",
    "\n",
    "        win_size_enc = win_size + 1  # additional 1 is to pass the true token apart from the context of win_size\n",
    "        bsz = 2048\n",
    "\n",
    "        ranks_list = []\n",
    "        probs_tok_list = []\n",
    "\n",
    "        n_runs = tokens_full.size-win_size_enc+1\n",
    "\n",
    "        tokens_encoded = tokens_full[win_size:win_size+n_runs]\n",
    "        print(win_size)\n",
    "        self.starter_tokens = tokens_full[:win_size]\n",
    "        np.save(self.filename + '_starter_tokens.npy', self.starter_tokens)\n",
    "\n",
    "        n_batches = np.ceil(n_runs/bsz).astype(int)\n",
    "\n",
    "        for b_ind in range(n_batches):\n",
    "            batch_range_start = b_ind*bsz\n",
    "            batch_range_stop = np.minimum(n_runs, (b_ind+1) * bsz)\n",
    "            # tokens_batch = np.array([np.concatenate(([tokens_full[0]], tokens_full[i:i+win_size_enc])) for i in range(batch_range_start, batch_range_stop)])\n",
    "            tokens_batch = np.array([tokens_full[i: i + win_size_enc] for i in range(batch_range_start, batch_range_stop)])\n",
    "            ranks, probs_tok = self.encode_batch(tokens_batch)\n",
    "            ranks_list += [ranks]\n",
    "            probs_tok_list += [probs_tok]\n",
    "\n",
    "            if (b_ind*bsz*100/n_batches) % 10 == 0:\n",
    "                print(f'Encoder: Completed {int(b_ind*bsz*100/n_batches)} %')\n",
    "\n",
    "        ranks_full = np.concatenate(ranks_list, 0).squeeze()\n",
    "        probs_tok_full = np.concatenate(probs_tok_list, 0).squeeze()\n",
    "\n",
    "        self.token_length = len(tokens_encoded)\n",
    "\n",
    "        self.AC_encoder.finish()\n",
    "        self.bitout.close()\n",
    "        self.file_out.close()\n",
    "\n",
    "        self.compute_compression_ratio(tokens_encoded, probs_tok_full)\n",
    "\n",
    "    def decode(self, win_size: int):\n",
    "        # Open the compressed bit-stream\n",
    "        with open(self.filename + '_llmzip_ac.txt', 'rb') as f_in:\n",
    "            bitin   = BitInputStream(f_in)\n",
    "            decoder = ArithmeticDecoder(32, bitin)\n",
    "\n",
    "            # Load initial window\n",
    "            decoded = list(self.starter_tokens[:win_size])\n",
    "            n_to_decode = self.token_length\n",
    "\n",
    "            # Loop through tokens\n",
    "            for _ in range(n_to_decode):\n",
    "                # build context from win_size tokens\n",
    "                ctx = torch.tensor([decoded[-win_size:]])\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(ctx).logits[:, -1, :]\n",
    "                    probs  = torch.softmax(logits, dim=-1).numpy()[0]\n",
    "\n",
    "                # rebuild distribution for the next token\n",
    "                cumul = np.zeros(probs.shape[0] + 1, dtype=np.uint64)\n",
    "                cumul[1:] = np.cumsum(probs * 10_000_000 + 1)\n",
    "\n",
    "                # pull one symbol out of the bit-stream\n",
    "                sym = decoder.read(cumul, alphabet_size=self.alphabet_size)\n",
    "                decoded.append(int(sym))\n",
    "\n",
    "        # convert to text\n",
    "        return self.tokenizer.decode(decoded)\n",
    "\n",
    "    def compute_compression_ratio(self, tokens_encoded, probs_tok):\n",
    "        text_encoded = self.tokenizer.decode(tokens_encoded.squeeze().tolist())\n",
    "\n",
    "        N_T = tokens_encoded.size\n",
    "        N_C = len(text_encoded)\n",
    "\n",
    "        df_out = {}\n",
    "        df_out['characters'] = N_C\n",
    "        df_out['tokens'] = N_T\n",
    "\n",
    "        entropy_val = np.sum(-np.log2(probs_tok)) / N_C\n",
    "        df_out['entropy'] = [f\"{entropy_val:.4f}\"]\n",
    "\n",
    "        file_in = open(self.filename+\"_llmzip_ac.txt\", 'rb')\n",
    "        bitin = BitInputStream(file_in)\n",
    "        compressed_bits = read_bitstream(bitin)\n",
    "        rho_AC = compressed_bits.size/N_C\n",
    "        print(f'Compression Ratio for Arithmetic Coding :  {rho_AC} bits/char')\n",
    "        file_in.close()\n",
    "\n",
    "        df_out['Llama+AC compressed file size'] = compressed_bits.size\n",
    "        df_out['bits per character'] = rho_AC\n",
    "\n",
    "        print(df_out)\n",
    "\n",
    "        with open(self.filename+'_metrics.json', 'w') as file_metrics:\n",
    "            json.dump(df_out, file_metrics)\n",
    "\n",
    "\n",
    "Encoder = LLMzip_encode(model, tokenizer, filename='../test.txt')\n",
    "\n",
    "Encoder.encode(win_size=13)\n",
    "recovered_text = Encoder.decode(win_size=13)\n",
    "print(\"Recovered text:\", recovered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../text8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m Encoder = LLMzip_encode(model, tokenizer, filename=\u001b[33m'\u001b[39m\u001b[33m../text8.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwin_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mLLMzip_encode.encode\u001b[39m\u001b[34m(self, win_size)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, win_size: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[38;5;28mself\u001b[39m.filename + \u001b[33m'\u001b[39m\u001b[33m_tokens.npy\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[32m    100\u001b[39m             text_input = f_in.read()\n\u001b[32m    102\u001b[39m         tokens_full = np.array(tokenizer.encode(text_input))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../text8.txt'"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model, tokenizer, filename='../text8.txt')\n",
    "\n",
    "Encoder.encode(win_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_data = \"1. 子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」\"\n",
    "# string_data = \"蓋聞天地之數，有十二萬九千六百歲為一元。將一元分為十二會，乃子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥之十二支也。\"\n",
    "string_data = \"Alice was beginning to get very tired of sitting by her sister on the bank\"\n",
    "# string_data = \"En 1815, M. Charles-François-Bienvenu Myriel était évêque de Digne. C'était un vieillard d'environ soixante-quinze ans; il occupait le siège de Digne depuis 1806.\"\n",
    "top_k = 10\n",
    "\n",
    "stored_data = []\n",
    "prompt = \"\"\n",
    "i = 0\n",
    "while i < len(string_data):\n",
    "    print(f\"\\n\\nCurrent index: {i}\")\n",
    "    print(f\"Prompt ({i}): {prompt}\")\n",
    "    print(f\"stored_data ({i}): {stored_data}\")\n",
    "    if prompt == \"\":\n",
    "        prompt = string_data[i]\n",
    "        stored_data.append(string_data[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        top_tokens, top_values = top_k_token(prompt, k=top_k)\n",
    "        for j, (token, score) in enumerate(zip(top_tokens, top_values)):\n",
    "            print(f\"{j+1}. '{token}' (logit: {score.item():.4f})\")\n",
    "            token_len = len(token)\n",
    "            print(f\"\\tToken length: {token_len}\")\n",
    "            print(f\"\\tComparing with string_data[{i}:{i+token_len}] = '{string_data[i:i+token_len]}'\")\n",
    "            if token == string_data[i:i+token_len]:\n",
    "                stored_data.append(j)\n",
    "                prompt += token\n",
    "                i += token_len - 1\n",
    "                break\n",
    "            elif j == top_k - 1:\n",
    "                print(f\"Character '{string_data[i]}' not found in top {top_k} tokens.\")\n",
    "                stored_data.append(string_data[i])\n",
    "                prompt += string_data[i]\n",
    "        i += 1\n",
    "        # if i == 6:\n",
    "        #     break\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal store_data: {stored_data}\")\n",
    "print(f\"original string_data: {string_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
