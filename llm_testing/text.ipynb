{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7f45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19428084 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Total number of tokens: 19428084\n",
      "Number of distinct tokens: 35844\n"
     ]
    }
   ],
   "source": [
    "from prediction import TokenPredictor\n",
    "from llm_compressor import LLMCompressor\n",
    "import numpy as np\n",
    "# This script initializes a TokenPredictor with a specified data path.\n",
    "data_path = '../data/text8'\n",
    "token_predictor = TokenPredictor(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760c1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Text:  anarch\n",
      "Next token will be 2142\n",
      "Index of next token in top ids: 34544\n",
      "probabilities of the correct token: 0.005449335090816021\n",
      "Actial token: 94296\n",
      "Prompt Text:  anarchism\n",
      "Next token will be 43753\n",
      "Index of next token in top ids: 16807\n",
      "probabilities of the correct token: 0.00030313548631966114\n",
      "Actial token: 37330\n",
      "Prompt Text:  anarchism originated\n",
      "Next token will be 438\n",
      "Index of next token in top ids: 35684\n",
      "probabilities of the correct token: 0.053930994123220444\n",
      "Actial token: 98592\n",
      "Prompt Text:  anarchism originated as\n",
      "Next token will be 264\n",
      "Index of next token in top ids: 35812\n",
      "probabilities of the correct token: 0.7700403928756714\n",
      "Actial token: 99113\n",
      "Prompt Text:  anarchism originated as a\n",
      "Next token will be 4647\n",
      "Index of next token in top ids: 33023\n",
      "probabilities of the correct token: 0.0007639548857696354\n",
      "Actial token: 88845\n",
      "Prompt Text:  anarchism originated as a term\n",
      "Next token will be 315\n",
      "Index of next token in top ids: 35769\n",
      "probabilities of the correct token: 0.1346130073070526\n",
      "Actial token: 98943\n",
      "Prompt Text:  anarchism originated as a term of\n",
      "Next token will be 11480\n",
      "Index of next token in top ids: 29535\n",
      "probabilities of the correct token: 0.30472537875175476\n",
      "Actial token: 76711\n",
      "Prompt Text:  anarchism originated as a term of abuse\n",
      "Next token will be 1156\n",
      "Index of next token in top ids: 35186\n",
      "probabilities of the correct token: 0.0008475618669763207\n",
      "Actial token: 96741\n",
      "Prompt Text:  anarchism originated as a term of abuse first\n",
      "Next token will be 1483\n",
      "Index of next token in top ids: 34979\n",
      "probabilities of the correct token: 0.5289222002029419\n",
      "Actial token: 95983\n"
     ]
    }
   ],
   "source": [
    "data = token_predictor.get_data_tokens()[:10]\n",
    "prompt_tokens = []\n",
    "import numpy as np\n",
    "\n",
    "llm_compressor = LLMCompressor()\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    # pop the first token if the prompt is too long\n",
    "    if len(prompt_tokens) > 130000:\n",
    "        prompt_tokens.pop()\n",
    "    # add the next token to the prompt\n",
    "    prompt_tokens.append(data[i-1])\n",
    "    # print the current status in one line\n",
    "    # print(f\"\\rProcessing token {i}/{len(data)}\", end='')\n",
    "    # print(f\"\\nprompt_tokens: {prompt_tokens}\")\n",
    "    print(f\"Prompt Text: {token_predictor.detokenize(prompt_tokens)}\")\n",
    "    print(f\"Next token will be {data[i]}\")\n",
    "    next_token_index, probs_values = token_predictor.get_ids_probs(prompt_tokens, data[i])\n",
    "    probs_values = np.array(probs_values)\n",
    "    llm_compressor.next_token(next_token_index, probs_values)\n",
    "    print(f\"Index of next token in top ids: {next_token_index}\")\n",
    "    print(f\"probabilities of the correct token: {probs_values[next_token_index]}\")\n",
    "    print(f\"Actial token: {token_predictor.get_token_by_id(next_token_index)}\")\n",
    "\n",
    "bit_string = llm_compressor.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c553b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompress_prompt_tokens: [43465]\n",
      "decompress_prompt_tokens: [43465, 94296]\n",
      "Original token: [43465, 2142]\n",
      "decompress_prompt_tokens: [43465, 94296]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330]\n",
      "Original token: [43465, 2142, 43753]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592]\n",
      "Original token: [43465, 2142, 43753, 438]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113]\n",
      "Original token: [43465, 2142, 43753, 438, 264]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253, 315]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253, 315, 279]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253, 315, 279, 41193]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903, 73107]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253, 315, 279, 41193, 13791]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903, 73107]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903, 73107, 59166]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253, 315, 279, 41193, 13791, 23856]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903, 73107, 59166]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903, 73107, 59166, 99048]\n",
      "Original token: [43465, 2142, 43753, 438, 264, 4647, 315, 11480, 1156, 1483, 2348, 4124, 3238, 536, 73256, 2670, 279, 4078, 10637, 315, 279, 28963, 13791, 323, 279, 15510, 11695, 48253, 315, 279, 41193, 13791, 23856, 279]\n",
      "decompress_prompt_tokens: [43465, 94296, 37330, 98592, 99113, 88845, 98943, 76711, 96741, 95983, 93768, 89857, 91790, 98290, 13689, 93035, 99048, 89949, 78087, 98943, 99048, 52884, 73107, 98934, 99048, 70474, 76287, 33140, 98943, 99048, 39903, 73107, 59166, 99048]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     decompress_prompt_tokens\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompress_prompt_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecompress_prompt_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m _, _, probs_values \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecompress_prompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m probs_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(probs_values)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# if i == 1:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     print(probs_values[:10])\u001b[39;00m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/llm_testing/prediction.py:31\u001b[0m, in \u001b[0;36mTokenPredictor.get_token_info\u001b[0;34m(self, prompt_tokens)\u001b[0m\n\u001b[1;32m     29\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([prompt_tokens])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# shape: (1, vocab_size)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# shape: (vocab_size,)\u001b[39;00m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    699\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    700\u001b[0m )\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:436\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    434\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 436\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:257\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:161\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    160\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    163\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    164\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/lit2425/humanize/summer-offsite/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# decompress\n",
    "from llm_compressor import LLMDecompressor\n",
    "decompressor = LLMDecompressor(bit_string)\n",
    "decompress_prompt_tokens = []\n",
    "decompress_prompt_tokens.append(data[0])\n",
    "for i in range(1, len(data)):\n",
    "    if len(decompress_prompt_tokens) > 130000:\n",
    "        decompress_prompt_tokens.pop()\n",
    "    print(f\"decompress_prompt_tokens: {decompress_prompt_tokens}\")\n",
    "    _, _, probs_values = token_predictor.get_token_info(decompress_prompt_tokens)\n",
    "    probs_values = np.array(probs_values)\n",
    "    # if i == 1:\n",
    "    #     print(probs_values[:10])\n",
    "    token_idx = decompressor.decompress(probs_values)\n",
    "    # print(token_idx, data[i], token_predictor.get_token_by_id(token_idx))\n",
    "    decompress_prompt_tokens.append(token_predictor.get_token_by_id(token_idx))\n",
    "    # print(f\"\\rDecompressing token {i+1}/{len(data)}\", end='')\n",
    "    print(f\"decompress_prompt_tokens: {decompress_prompt_tokens}\")\n",
    "    print(f\"Original token: {data[:i+1]}\")\n",
    "    # assert decompress_prompt_tokens == data[:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550a15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bit_string)  # Length of the compressed bit string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f37c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n"
     ]
    }
   ],
   "source": [
    "output_string = token_predictor.detokenize(data)\n",
    "print(len(output_string))  # Length of the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655a0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.350844277673546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1496/(533*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9ddcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
