{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schmidt/summer-offsite/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\".cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\".cache\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_token(prompt, k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    topk_values, topk_indices = torch.topk(logits, top_k, dim=-1)\n",
    "    top_tokens = tokenizer.batch_decode(topk_indices[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    # print(topk_indices[0][:k])\n",
    "    \n",
    "    return top_tokens, topk_values[0]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_token_prop(prompt, k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    print(tokenizer.batch_decode(\n",
    "        inputs['input_ids'][0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    print(outputs)\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n",
    "\n",
    "    # Apply softmax to top-k logits to get probabilities\n",
    "    topk_probs = F.softmax(topk_values, dim=-1)\n",
    "\n",
    "    top_tokens = tokenizer.batch_decode(\n",
    "        topk_indices[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    return top_tokens, topk_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[4340,  646,  498]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "['How', ' can', ' you']\n",
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 8.2367,  6.6111,  4.5391,  ..., -4.0680, -4.0680, -4.0681],\n",
      "         [ 4.1064,  5.5320,  4.0867,  ..., -3.4824, -3.4814, -3.4826],\n",
      "         [ 6.9169,  8.1698,  9.0604,  ..., -3.9949, -3.9945, -3.9951]]]), past_key_values=<transformers.cache_utils.DynamicCache object at 0x72d228bfb320>, hidden_states=None, attentions=None)\n",
      "1.  determine (logit: 0.2247)\n",
      "2.  use (logit: 0.1334)\n",
      "3.  find (logit: 0.1147)\n",
      "4.  modify (logit: 0.1048)\n",
      "5.  simplify (logit: 0.0876)\n",
      "6.  calculate (logit: 0.0757)\n",
      "7.  solve (logit: 0.0678)\n",
      "8.  express (logit: 0.0650)\n",
      "9.  prove (logit: 0.0634)\n",
      "10.  create (logit: 0.0630)\n",
      "Sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How can you\"\n",
    "top_k = 10\n",
    "\n",
    "top_tokens, top_values = top_k_token_prop(prompt, k=top_k)\n",
    "for i, (token, score) in enumerate(zip(top_tokens, top_values)):\n",
    "    print(f\"{i+1}. {token} (logit: {score.item():.4f})\")\n",
    "print(f\"Sum: {top_values[0:10].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice| was| beginning| to| get| very| tired| of| sitting| by| her| sister| on| the| bank|.\n",
      "13\n",
      "AliceAlice was beginning to get very tired of sitting by her sister on the|Alice was beginning to get very tired of sitting by her sister on the bank|Alice beginning to get very tired of sitting by her sister on the bank.\n",
      "tensor([[61686, 61686,   572,  7167,   311,   633,  1602, 19227,   315, 11699,\n",
      "           553,  1059, 12923,   389,   279],\n",
      "        [61686,   572,  7167,   311,   633,  1602, 19227,   315, 11699,   553,\n",
      "          1059, 12923,   389,   279,  6073],\n",
      "        [61686,  7167,   311,   633,  1602, 19227,   315, 11699,   553,  1059,\n",
      "         12923,   389,   279,  6073,    13]])\n",
      " the| bank|.\n",
      "0.9674|0.2556|0.0982\n",
      "Encoder: Completed 0 %\n",
      "Compression Ratio for Arithmetic Coding :  0.6 bits/char\n",
      "{'characters': 10, 'tokens': 3, 'entropy': ['0.5365'], 'Llama+AC compressed file size': 6, 'bits per character': 0.6}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from arithmetic_coder import ArithmeticEncoder, BitInputStream, BitOutputStream\n",
    "\n",
    "\n",
    "def gen_rank(probs, next_token):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True, stable=True)\n",
    "    rank_list = []\n",
    "    if next_token.shape[0] > 1:\n",
    "        for i in range(next_token.shape[0]):\n",
    "            rank_list += [torch.where(probs_idx[i:i+1, :] == next_token[i])[-1]]\n",
    "        rank = torch.squeeze(torch.stack(rank_list))\n",
    "    else:\n",
    "        rank = torch.where(probs_idx == next_token)[-1]\n",
    "    return rank\n",
    "\n",
    "def read_bitstream(bitin):\n",
    "    temp_list = []\n",
    "    while True:\n",
    "        temp = bitin.read()\n",
    "        if temp == -1:\n",
    "            break\n",
    "        temp_list += [temp]\n",
    "    temp_arr = np.array(temp_list)\n",
    "    final_ind = (np.where(temp_arr==1)[0][-1]).astype(int)\n",
    "    final_arr = temp_arr[:final_ind+1]\n",
    "    \n",
    "    return final_arr\n",
    "\n",
    "def print_tokens(tokens):\n",
    "    decoded_tokens = tokenizer.batch_decode(\n",
    "        tokens,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    print(\"|\".join(decoded_tokens))\n",
    "\n",
    "def print_probs(probs):\n",
    "    print(\"|\".join([f\"{p:.4f}\" for p in probs]))\n",
    "\n",
    "class LLMzip_encode:\n",
    "    def __init__(self, model, tokenizer, filename):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.filename = filename\n",
    "        self.file_out = open(self.filename+'_llmzip_ac.txt', 'wb')\n",
    "        self.bitout = BitOutputStream(self.file_out)\n",
    "        self.AC_encoder = ArithmeticEncoder(32, self.bitout)\n",
    "\n",
    "    def encode_batch(self, prompt_tokens):\n",
    "        print_tokens(prompt_tokens)\n",
    "        bsz = prompt_tokens.shape[0]\n",
    "\n",
    "        prompt_size = prompt_tokens.shape[1]\n",
    "\n",
    "        tokens = torch.full((bsz, prompt_size), self.tokenizer.pad_token_id).long()\n",
    "        tokens[:bsz, : prompt_size] = torch.tensor(prompt_tokens).long()\n",
    "        print(tokens)\n",
    "\n",
    "        cur_pos = prompt_size-1\n",
    "        prev_pos = 0\n",
    "\n",
    "        logits = self.model.forward(tokens[:, prev_pos:cur_pos]).logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        rank = gen_rank(probs, next_token=tokens[:, cur_pos])\n",
    "\n",
    "        probs_np2 = probs.cpu().detach().numpy()\n",
    "        tokens_np2 = tokens[:, cur_pos].cpu().numpy()\n",
    "        ranks_np2 = rank.cpu().numpy()\n",
    "\n",
    "        probs_tok = probs_np2[np.arange(bsz), tokens_np2]\n",
    "        print_tokens(tokens_np2)\n",
    "        print_probs(probs_tok)\n",
    "\n",
    "        cumul = np.zeros(self.model.vocab_size+1, dtype=np.uint64)\n",
    "        for j in range(bsz):\n",
    "            prob1 = probs_np2[j]\n",
    "            cumul[1:] = np.cumsum(prob1*10000000 + 1)\n",
    "            self.AC_encoder.write(cumul, tokens_np2[j])\n",
    "\n",
    "        return ranks_np2, probs_tok\n",
    "\n",
    "    def encode(self, win_size: int):\n",
    "        with open(self.filename,'r') as f_in:\n",
    "            text_input = f_in.read()\n",
    "\n",
    "        tokens_full = np.array(tokenizer.encode(text_input))\n",
    "\n",
    "        print_tokens(tokens_full)\n",
    "\n",
    "        win_size_enc = win_size + 1  # additional 1 is to pass the true token apart from the context of win_size\n",
    "        bsz = 2048\n",
    "\n",
    "        ranks_list = []\n",
    "        probs_tok_list = []\n",
    "\n",
    "        n_runs = tokens_full.size-win_size_enc+1\n",
    "\n",
    "        tokens_encoded = tokens_full[win_size:win_size+n_runs]\n",
    "        print(win_size)\n",
    "        starter_tokens = tokens_full[:win_size]\n",
    "\n",
    "        n_batches = np.ceil(n_runs/bsz).astype(int)\n",
    "\n",
    "        for b_ind in range(n_batches):\n",
    "            batch_range_start = b_ind*bsz\n",
    "            batch_range_stop = np.minimum(n_runs, (b_ind+1) * bsz)\n",
    "            #tokens_batch = np.array([np.concatenate(([tokens_full[0]], tokens_full[i:i+win_size_enc])) for i in range(batch_range_start, batch_range_stop)])\n",
    "            tokens_batch = np.array([tokens_full[i:i+win_size_enc] for i in range(batch_range_start, batch_range_stop)])\n",
    "            ranks, probs_tok = self.encode_batch(tokens_batch)\n",
    "            ranks_list += [ranks]\n",
    "            probs_tok_list += [probs_tok]\n",
    "\n",
    "            if (b_ind*bsz*100/n_batches) % 10 == 0:\n",
    "                print(f'Encoder: Completed {int(b_ind*bsz*100/n_batches)} %')\n",
    "\n",
    "        ranks_full = np.concatenate(ranks_list, 0).squeeze()\n",
    "        probs_tok_full = np.concatenate(probs_tok_list, 0).squeeze()\n",
    "\n",
    "        self.AC_encoder.finish()\n",
    "        self.bitout.close()\n",
    "        self.file_out.close()\n",
    "\n",
    "        self.compute_compression_ratio(tokens_encoded, probs_tok_full)\n",
    "\n",
    "    def compute_compression_ratio(self, tokens_encoded, probs_tok):\n",
    "        text_encoded = self.tokenizer.decode(tokens_encoded.squeeze().tolist())\n",
    "\n",
    "        N_T = tokens_encoded.size\n",
    "        N_C = len(text_encoded)\n",
    "\n",
    "        df_out = {}\n",
    "        df_out['characters'] = N_C\n",
    "        df_out['tokens'] = N_T\n",
    "\n",
    "        entropy_val = np.sum(-np.log2(probs_tok)) / N_C\n",
    "        df_out['entropy'] = [f\"{entropy_val:.4f}\"]\n",
    "\n",
    "        file_in = open(self.filename+\"_llmzip_ac.txt\", 'rb')\n",
    "        bitin = BitInputStream(file_in)\n",
    "        compressed_bits = read_bitstream(bitin)\n",
    "        rho_AC = compressed_bits.size/N_C\n",
    "        print(f'Compression Ratio for Arithmetic Coding :  {rho_AC} bits/char')\n",
    "        file_in.close()\n",
    "\n",
    "        df_out['Llama+AC compressed file size'] = compressed_bits.size\n",
    "        df_out['bits per character'] = rho_AC\n",
    "\n",
    "        print(df_out)\n",
    "\n",
    "        with open(self.filename+'_metrics.json', 'w') as file_metrics:\n",
    "            json.dump(df_out, file_metrics)\n",
    "\n",
    "Encoder = LLMzip_encode(model, tokenizer, filename='../test.txt')\n",
    "\n",
    "Encoder.encode(win_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19428084 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model, tokenizer, filename='../text8.txt')\n",
    "\n",
    "Encoder.encode(win_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current index: 0\n",
      "Prompt (0): \n",
      "stored_data (0): []\n",
      "\n",
      "\n",
      "Current index: 1\n",
      "Prompt (1): A\n",
      "stored_data (1): ['A']\n",
      "1. ' high' (logit: 12.8682)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[1:6] = 'lice '\n",
      "2. ' ' (logit: 12.5429)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[1:2] = 'l'\n",
      "3. ' certain' (logit: 12.3691)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[1:9] = 'lice was'\n",
      "4. ' middle' (logit: 12.3153)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[1:8] = 'lice wa'\n",
      "5. ' company' (logit: 12.0925)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[1:9] = 'lice was'\n",
      "6. ' group' (logit: 12.0796)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[1:7] = 'lice w'\n",
      "7. ' poly' (logit: 11.8678)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[1:6] = 'lice '\n",
      "8. ' circle' (logit: 11.8444)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[1:8] = 'lice wa'\n",
      "9. ' rectangular' (logit: 11.7547)\n",
      "\tToken length: 12\n",
      "\tComparing with string_data[1:13] = 'lice was beg'\n",
      "10. ' square' (logit: 11.7440)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[1:8] = 'lice wa'\n",
      "Character 'l' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 2\n",
      "Prompt (2): Al\n",
      "stored_data (2): ['A', 'l']\n",
      "1. 'gebra' (logit: 16.0380)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[2:7] = 'ice w'\n",
      "2. 'fred' (logit: 14.2115)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[2:6] = 'ice '\n",
      "3. 'ison' (logit: 14.0822)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[2:6] = 'ice '\n",
      "4. 'cohol' (logit: 13.8838)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[2:7] = 'ice w'\n",
      "5. 'gorithms' (logit: 13.8360)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[2:10] = 'ice was '\n",
      "6. 'vin' (logit: 13.7853)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[2:5] = 'ice'\n",
      "7. 'zheimer' (logit: 13.6675)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[2:9] = 'ice was'\n",
      "8. 'fon' (logit: 13.6630)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[2:5] = 'ice'\n",
      "9. 'ma' (logit: 13.2554)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[2:4] = 'ic'\n",
      "10. 'aska' (logit: 13.2032)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[2:6] = 'ice '\n",
      "Character 'i' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 3\n",
      "Prompt (3): Ali\n",
      "stored_data (3): ['A', 'l', 'i']\n",
      "1. ' has' (logit: 14.5069)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[3:7] = 'ce w'\n",
      "2. ' and' (logit: 14.1880)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[3:7] = 'ce w'\n",
      "3. ' had' (logit: 14.1672)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[3:7] = 'ce w'\n",
      "4. ' is' (logit: 14.1145)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[3:6] = 'ce '\n",
      "5. ',' (logit: 14.0329)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[3:4] = 'c'\n",
      "6. 'en' (logit: 13.5066)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[3:5] = 'ce'\n",
      "7. 'ens' (logit: 13.0607)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[3:6] = 'ce '\n",
      "8. ''s' (logit: 12.7434)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[3:5] = 'ce'\n",
      "9. 'yah' (logit: 12.6552)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[3:6] = 'ce '\n",
      "10. 'ya' (logit: 12.5454)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[3:5] = 'ce'\n",
      "Character 'c' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 4\n",
      "Prompt (4): Alic\n",
      "stored_data (4): ['A', 'l', 'i', 'c']\n",
      "1. 'ia' (logit: 21.3737)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "2. 'ja' (logit: 16.1414)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "3. 'IA' (logit: 15.3902)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "4. 'ha' (logit: 13.4238)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "5. 'an' (logit: 12.9463)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "6. 'ía' (logit: 12.8533)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "7. 'is' (logit: 12.7289)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "8. 'ob' (logit: 12.3350)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "9. 'io' (logit: 12.2568)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "10. 'ya' (logit: 12.1520)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[4:6] = 'e '\n",
      "Character 'e' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 5\n",
      "Prompt (5): Alice\n",
      "stored_data (5): ['A', 'l', 'i', 'c', 'e']\n",
      "1. ' and' (logit: 17.7167)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[5:9] = ' was'\n",
      "2. ' is' (logit: 16.9922)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[5:8] = ' wa'\n",
      "3. ' has' (logit: 16.2017)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[5:9] = ' was'\n",
      "4. ',' (logit: 15.5963)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[5:6] = ' '\n",
      "5. ' 和' (logit: 14.5944)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[5:7] = ' w'\n",
      "6. '：' (logit: 14.5777)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[5:6] = ' '\n",
      "7. ':' (logit: 13.9919)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[5:6] = ' '\n",
      "8. ' ' (logit: 13.9618)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[5:6] = ' '\n",
      "\n",
      "\n",
      "Current index: 6\n",
      "Prompt (6): Alice \n",
      "stored_data (6): ['A', 'l', 'i', 'c', 'e', 7]\n",
      "1. '有' (logit: 18.1813)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[6:7] = 'w'\n",
      "2. '有一个' (logit: 16.1410)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[6:9] = 'was'\n",
      "3. '有一' (logit: 15.8811)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[6:8] = 'wa'\n",
      "4. '以' (logit: 13.3643)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[6:7] = 'w'\n",
      "5. '有着' (logit: 13.0533)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[6:8] = 'wa'\n",
      "6. '有两个' (logit: 12.8107)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[6:9] = 'was'\n",
      "7. '无' (logit: 12.3435)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[6:7] = 'w'\n",
      "8. ' 开' (logit: 12.2951)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[6:8] = 'wa'\n",
      "9. ' has' (logit: 12.2814)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[6:10] = 'was '\n",
      "10. ' ' (logit: 12.2665)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[6:7] = 'w'\n",
      "Character 'w' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 7\n",
      "Prompt (7): Alice w\n",
      "stored_data (7): ['A', 'l', 'i', 'c', 'e', 7, 'w']\n",
      "1. 'eds' (logit: 18.1189)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[7:10] = 'as '\n",
      "2. 'ets' (logit: 17.2848)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[7:10] = 'as '\n",
      "3. 'agers' (logit: 16.6553)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[7:12] = 'as be'\n",
      "4. 'ades' (logit: 16.0878)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[7:11] = 'as b'\n",
      "5. 'itt' (logit: 15.3934)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[7:10] = 'as '\n",
      "6. 'aded' (logit: 15.3371)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[7:11] = 'as b'\n",
      "7. 'oted' (logit: 15.2556)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[7:11] = 'as b'\n",
      "8. 'add' (logit: 15.0827)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[7:10] = 'as '\n",
      "9. 'owed' (logit: 14.5370)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[7:11] = 'as b'\n",
      "10. ' subtract' (logit: 14.2458)\n",
      "\tToken length: 9\n",
      "\tComparing with string_data[7:16] = 'as beginn'\n",
      "Character 'a' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 8\n",
      "Prompt (8): Alice wa\n",
      "stored_data (8): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a']\n",
      "1. 'ives' (logit: 17.1456)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[8:12] = 's be'\n",
      "2. 'fts' (logit: 16.7608)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[8:11] = 's b'\n",
      "3. 'its' (logit: 15.9752)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[8:11] = 's b'\n",
      "4. ' has' (logit: 14.7018)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[8:12] = 's be'\n",
      "5. 'ft' (logit: 14.0320)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[8:10] = 's '\n",
      "6. 'ists' (logit: 14.0281)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[8:12] = 's be'\n",
      "7. 'iving' (logit: 13.9020)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[8:13] = 's beg'\n",
      "8. ' is' (logit: 13.4539)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[8:11] = 's b'\n",
      "9. 'ifs' (logit: 13.0933)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[8:11] = 's b'\n",
      "10. ' and' (logit: 13.0342)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[8:12] = 's be'\n",
      "Character 's' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 9\n",
      "Prompt (9): Alice was\n",
      "stored_data (9): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's']\n",
      "1. ' able' (logit: 17.3966)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[9:14] = ' begi'\n",
      "2. ' playing' (logit: 14.3079)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[9:17] = ' beginni'\n",
      "3. ' given' (logit: 13.8280)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[9:15] = ' begin'\n",
      "4. ' with' (logit: 13.7613)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[9:14] = ' begi'\n",
      "5. ' buying' (logit: 13.5025)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[9:16] = ' beginn'\n",
      "6. ' saving' (logit: 13.4235)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[9:16] = ' beginn'\n",
      "7. ' going' (logit: 13.2874)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[9:15] = ' begin'\n",
      "8. ' planning' (logit: 13.2338)\n",
      "\tToken length: 9\n",
      "\tComparing with string_data[9:18] = ' beginnin'\n",
      "9. ' careful' (logit: 12.7293)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[9:17] = ' beginni'\n",
      "10. ' making' (logit: 12.7048)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[9:16] = ' beginn'\n",
      "Character ' ' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 10\n",
      "Prompt (10): Alice was \n",
      "stored_data (10): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ']\n",
      "1. '2' (logit: 24.5191)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "2. '1' (logit: 24.3627)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "3. '3' (logit: 24.3553)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "4. '5' (logit: 23.8558)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "5. '9' (logit: 23.6977)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "6. '4' (logit: 23.3574)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "7. '8' (logit: 22.7118)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "8. '6' (logit: 22.5596)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "9. '7' (logit: 22.0385)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "10. '0' (logit: 21.4131)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[10:11] = 'b'\n",
      "Character 'b' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 11\n",
      "Prompt (11): Alice was b\n",
      "stored_data (11): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b']\n",
      "1. 'asking' (logit: 16.0206)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[11:17] = 'eginni'\n",
      "2. 'aking' (logit: 15.1267)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[11:16] = 'eginn'\n",
      "3. 'akes' (logit: 15.1112)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[11:15] = 'egin'\n",
      "4. 'reading' (logit: 14.6439)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[11:18] = 'eginnin'\n",
      "5. 'iding' (logit: 13.6191)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[11:16] = 'eginn'\n",
      "6. 'ailing' (logit: 13.6043)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[11:17] = 'eginni'\n",
      "7. 'ount' (logit: 13.5945)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[11:15] = 'egin'\n",
      "8. 'eth' (logit: 12.9526)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[11:14] = 'egi'\n",
      "9. 'arking' (logit: 12.3860)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[11:17] = 'eginni'\n",
      "10. 'achel' (logit: 12.1973)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[11:16] = 'eginn'\n",
      "Character 'e' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 12\n",
      "Prompt (12): Alice was be\n",
      "stored_data (12): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e']\n",
      "1. 'headed' (logit: 17.0832)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[12:18] = 'ginnin'\n",
      "2. 'aming' (logit: 16.7750)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[12:17] = 'ginni'\n",
      "3. 'que' (logit: 16.7311)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[12:15] = 'gin'\n",
      "4. 'heading' (logit: 16.0598)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[12:19] = 'ginning'\n",
      "5. 'ading' (logit: 14.8669)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[12:17] = 'ginni'\n",
      "6. 'ating' (logit: 14.8575)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[12:17] = 'ginni'\n",
      "7. 'getting' (logit: 14.7099)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[12:19] = 'ginning'\n",
      "8. 'eh' (logit: 14.2320)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[12:14] = 'gi'\n",
      "9. 'ater' (logit: 13.9910)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[12:16] = 'ginn'\n",
      "10. ' a' (logit: 13.9212)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[12:14] = 'gi'\n",
      "Character 'g' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 13\n",
      "Prompt (13): Alice was beg\n",
      "stored_data (13): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g']\n",
      "1. 'u' (logit: 19.1278)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[13:14] = 'i'\n",
      "2. 'g' (logit: 15.5214)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[13:14] = 'i'\n",
      "3. 'gar' (logit: 15.1214)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[13:16] = 'inn'\n",
      "4. 'inging' (logit: 15.0341)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[13:19] = 'inning'\n",
      "5. 'un' (logit: 14.9120)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[13:15] = 'in'\n",
      "6. 'ing' (logit: 14.0310)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[13:16] = 'inn'\n",
      "7. 'at' (logit: 13.7608)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[13:15] = 'in'\n",
      "8. 'b' (logit: 13.7020)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[13:14] = 'i'\n",
      "9. 'otten' (logit: 13.5624)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[13:18] = 'innin'\n",
      "10. 'i' (logit: 13.5123)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[13:14] = 'i'\n",
      "\n",
      "\n",
      "Current index: 14\n",
      "Prompt (14): Alice was begi\n",
      "stored_data (14): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9]\n",
      "1. 'ing' (logit: 17.1334)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[14:17] = 'nni'\n",
      "2. ' n' (logit: 16.2865)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[14:16] = 'nn'\n",
      "3. 'ining' (logit: 15.8134)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[14:19] = 'nning'\n",
      "4. '...' (logit: 15.2568)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[14:17] = 'nni'\n",
      "5. '...\n",
      "\n",
      "' (logit: 15.0877)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[14:19] = 'nning'\n",
      "6. '\n",
      "' (logit: 15.0828)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[14:15] = 'n'\n",
      "7. '...\\' (logit: 15.0768)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[14:18] = 'nnin'\n",
      "8. '\n",
      "\n",
      "' (logit: 14.8714)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[14:16] = 'nn'\n",
      "9. ' -' (logit: 14.6952)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[14:16] = 'nn'\n",
      "10. '-' (logit: 14.6359)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[14:15] = 'n'\n",
      "Character 'n' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 15\n",
      "Prompt (15): Alice was begin\n",
      "stored_data (15): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n']\n",
      "1. 'ing' (logit: 19.5411)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[15:18] = 'nin'\n",
      "2. 'n' (logit: 17.7997)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[15:16] = 'n'\n",
      "\n",
      "\n",
      "Current index: 16\n",
      "Prompt (16): Alice was beginn\n",
      "stored_data (16): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1]\n",
      "1. 'ning' (logit: 19.3434)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[16:20] = 'ing '\n",
      "2. 'ig' (logit: 18.1493)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[16:18] = 'in'\n",
      "3. 'ign' (logit: 17.2206)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[16:19] = 'ing'\n",
      "4. 'ng' (logit: 16.5793)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[16:18] = 'in'\n",
      "5. ' ing' (logit: 16.3982)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[16:20] = 'ing '\n",
      "6. 'ag' (logit: 15.9372)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[16:18] = 'in'\n",
      "7. 'ining' (logit: 15.9271)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[16:21] = 'ing t'\n",
      "8. '-ing' (logit: 14.6000)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[16:20] = 'ing '\n",
      "9. '\n",
      "' (logit: 14.3845)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[16:17] = 'i'\n",
      "10. '\n",
      "\n",
      "' (logit: 14.2297)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[16:18] = 'in'\n",
      "Character 'i' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 17\n",
      "Prompt (17): Alice was beginni\n",
      "stored_data (17): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i']\n",
      "1. 'ing' (logit: 17.2219)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[17:20] = 'ng '\n",
      "2. ' ng' (logit: 16.2410)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[17:20] = 'ng '\n",
      "3. 'ug' (logit: 16.0083)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[17:19] = 'ng'\n",
      "4. '\n",
      "\n",
      "' (logit: 14.0497)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[17:19] = 'ng'\n",
      "5. 'NG' (logit: 13.6794)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[17:19] = 'ng'\n",
      "6. 'ng' (logit: 13.6143)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[17:19] = 'ng'\n",
      "\n",
      "\n",
      "Current index: 19\n",
      "Prompt (19): Alice was beginning\n",
      "stored_data (19): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5]\n",
      "1. ' to' (logit: 21.1822)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[19:22] = ' to'\n",
      "\n",
      "\n",
      "Current index: 22\n",
      "Prompt (22): Alice was beginning to\n",
      "stored_data (22): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0]\n",
      "1. ' write' (logit: 13.5821)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[22:28] = ' get v'\n",
      "2. ' feel' (logit: 13.2862)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[22:27] = ' get '\n",
      "3. ' lose' (logit: 13.2669)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[22:27] = ' get '\n",
      "4. ' count' (logit: 12.7337)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[22:28] = ' get v'\n",
      "5. ' plot' (logit: 12.6252)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[22:27] = ' get '\n",
      "6. ' construct' (logit: 12.2200)\n",
      "\tToken length: 10\n",
      "\tComparing with string_data[22:32] = ' get very '\n",
      "7. ' compute' (logit: 12.1281)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[22:30] = ' get ver'\n",
      "8. ' despair' (logit: 11.8777)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[22:30] = ' get ver'\n",
      "9. ' trouble' (logit: 11.8216)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[22:30] = ' get ver'\n",
      "10. ' lament' (logit: 11.7663)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[22:29] = ' get ve'\n",
      "Character ' ' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 23\n",
      "Prompt (23): Alice was beginning to \n",
      "stored_data (23): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ']\n",
      "1. '1' (logit: 17.0854)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "2. '2' (logit: 16.4069)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "3. '8' (logit: 16.3507)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "4. '5' (logit: 15.9578)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "5. '7' (logit: 15.6946)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "6. '9' (logit: 15.6863)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "7. '3' (logit: 15.5943)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "8. '4' (logit: 15.5738)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "9. '6' (logit: 15.4771)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[23:24] = 'g'\n",
      "10. ' get' (logit: 15.1853)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[23:27] = 'get '\n",
      "Character 'g' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 24\n",
      "Prompt (24): Alice was beginning to g\n",
      "stored_data (24): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g']\n",
      "1. 'rieve' (logit: 19.2189)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[24:29] = 'et ve'\n",
      "2. '____' (logit: 16.4161)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[24:28] = 'et v'\n",
      "3. ' ____' (logit: 16.2968)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[24:29] = 'et ve'\n",
      "4. 'ird' (logit: 15.2147)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[24:27] = 'et '\n",
      "5. ' _____' (logit: 14.3167)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[24:30] = 'et ver'\n",
      "6. 'listen' (logit: 14.2988)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[24:30] = 'et ver'\n",
      "7. 'rouch' (logit: 14.0835)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[24:29] = 'et ve'\n",
      "8. 'loat' (logit: 14.0734)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[24:28] = 'et v'\n",
      "9. ' __' (logit: 13.9936)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[24:27] = 'et '\n",
      "10. 'ret' (logit: 13.9344)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[24:27] = 'et '\n",
      "Character 'e' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 25\n",
      "Prompt (25): Alice was beginning to ge\n",
      "stored_data (25): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e']\n",
      "1. '-' (logit: 14.5284)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[25:26] = 't'\n",
      "2. 'orge' (logit: 14.4548)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[25:29] = 't ve'\n",
      "3. '____' (logit: 14.1700)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[25:29] = 't ve'\n",
      "4. ' n' (logit: 14.0363)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[25:27] = 't '\n",
      "5. 'ographically' (logit: 13.8087)\n",
      "\tToken length: 12\n",
      "\tComparing with string_data[25:37] = 't very tired'\n",
      "6. '\n",
      "' (logit: 13.7943)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[25:26] = 't'\n",
      "7. '­' (logit: 13.6054)\n",
      "\tToken length: 1\n",
      "\tComparing with string_data[25:26] = 't'\n",
      "8. ' -' (logit: 13.4069)\n",
      "\tToken length: 2\n",
      "\tComparing with string_data[25:27] = 't '\n",
      "9. 'ese' (logit: 13.3316)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[25:28] = 't v'\n",
      "10. 'ology' (logit: 13.2854)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[25:30] = 't ver'\n",
      "Character 't' not found in top 10 tokens.\n",
      "\n",
      "\n",
      "Current index: 26\n",
      "Prompt (26): Alice was beginning to get\n",
      "stored_data (26): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't']\n",
      "1. ' very' (logit: 16.6216)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[26:31] = ' very'\n",
      "\n",
      "\n",
      "Current index: 31\n",
      "Prompt (31): Alice was beginning to get very\n",
      "stored_data (31): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0]\n",
      "1. ' tired' (logit: 16.9335)\n",
      "\tToken length: 6\n",
      "\tComparing with string_data[31:37] = ' tired'\n",
      "\n",
      "\n",
      "Current index: 37\n",
      "Prompt (37): Alice was beginning to get very tired\n",
      "stored_data (37): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0]\n",
      "1. ' of' (logit: 18.1285)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[37:40] = ' of'\n",
      "\n",
      "\n",
      "Current index: 40\n",
      "Prompt (40): Alice was beginning to get very tired of\n",
      "stored_data (40): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0]\n",
      "1. ' standing' (logit: 20.0368)\n",
      "\tToken length: 9\n",
      "\tComparing with string_data[40:49] = ' sitting '\n",
      "2. ' sitting' (logit: 16.5812)\n",
      "\tToken length: 8\n",
      "\tComparing with string_data[40:48] = ' sitting'\n",
      "\n",
      "\n",
      "Current index: 48\n",
      "Prompt (48): Alice was beginning to get very tired of sitting\n",
      "stored_data (48): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1]\n",
      "1. ' by' (logit: 19.2033)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[48:51] = ' by'\n",
      "\n",
      "\n",
      "Current index: 51\n",
      "Prompt (51): Alice was beginning to get very tired of sitting by\n",
      "stored_data (51): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1, 0]\n",
      "1. ' the' (logit: 17.4468)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[51:55] = ' her'\n",
      "2. ' her' (logit: 16.1627)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[51:55] = ' her'\n",
      "\n",
      "\n",
      "Current index: 55\n",
      "Prompt (55): Alice was beginning to get very tired of sitting by her\n",
      "stored_data (55): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1, 0, 1]\n",
      "1. ' sister' (logit: 19.7719)\n",
      "\tToken length: 7\n",
      "\tComparing with string_data[55:62] = ' sister'\n",
      "\n",
      "\n",
      "Current index: 62\n",
      "Prompt (62): Alice was beginning to get very tired of sitting by her sister\n",
      "stored_data (62): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1, 0, 1, 0]\n",
      "1. ' on' (logit: 15.5985)\n",
      "\tToken length: 3\n",
      "\tComparing with string_data[62:65] = ' on'\n",
      "\n",
      "\n",
      "Current index: 65\n",
      "Prompt (65): Alice was beginning to get very tired of sitting by her sister on\n",
      "stored_data (65): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "1. ' the' (logit: 18.4984)\n",
      "\tToken length: 4\n",
      "\tComparing with string_data[65:69] = ' the'\n",
      "\n",
      "\n",
      "Current index: 69\n",
      "Prompt (69): Alice was beginning to get very tired of sitting by her sister on the\n",
      "stored_data (69): ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "1. ' bank' (logit: 13.8467)\n",
      "\tToken length: 5\n",
      "\tComparing with string_data[69:74] = ' bank'\n"
     ]
    }
   ],
   "source": [
    "# string_data = \"1. 子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」\"\n",
    "# string_data = \"蓋聞天地之數，有十二萬九千六百歲為一元。將一元分為十二會，乃子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥之十二支也。\"\n",
    "string_data = \"Alice was beginning to get very tired of sitting by her sister on the bank\"\n",
    "# string_data = \"En 1815, M. Charles-François-Bienvenu Myriel était évêque de Digne. C'était un vieillard d'environ soixante-quinze ans; il occupait le siège de Digne depuis 1806.\"\n",
    "top_k = 10\n",
    "\n",
    "stored_data = []\n",
    "prompt = \"\"\n",
    "i = 0\n",
    "while i < len(string_data):\n",
    "    print(f\"\\n\\nCurrent index: {i}\")\n",
    "    print(f\"Prompt ({i}): {prompt}\")\n",
    "    print(f\"stored_data ({i}): {stored_data}\")\n",
    "    if prompt == \"\":\n",
    "        prompt = string_data[i]\n",
    "        stored_data.append(string_data[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        top_tokens, top_values = top_k_token(prompt, k=top_k)\n",
    "        for j, (token, score) in enumerate(zip(top_tokens, top_values)):\n",
    "            print(f\"{j+1}. '{token}' (logit: {score.item():.4f})\")\n",
    "            token_len = len(token)\n",
    "            print(f\"\\tToken length: {token_len}\")\n",
    "            print(f\"\\tComparing with string_data[{i}:{i+token_len}] = '{string_data[i:i+token_len]}'\")\n",
    "            if token == string_data[i:i+token_len]:\n",
    "                stored_data.append(j)\n",
    "                prompt += token\n",
    "                i += token_len - 1\n",
    "                break\n",
    "            elif j == top_k - 1:\n",
    "                print(f\"Character '{string_data[i]}' not found in top {top_k} tokens.\")\n",
    "                stored_data.append(string_data[i])\n",
    "                prompt += string_data[i]\n",
    "        i += 1\n",
    "        # if i == 6:\n",
    "        #     break\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final store_data: ['A', 'l', 'i', 'c', 'e', 7, 'w', 'a', 's', ' ', 'b', 'e', 'g', 9, 'n', 1, 'i', 5, 0, ' ', 'g', 'e', 't', 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "original string_data: Alice was beginning to get very tired of sitting by her sister on the bank\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFinal store_data: {stored_data}\")\n",
    "print(f\"original string_data: {string_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
