{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /root/summer-offsite/.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /root/summer-offsite/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /root/summer-offsite/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /root/summer-offsite/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /root/summer-offsite/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing ipywidgets...\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Installing ipywidgets...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\".cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\".cache\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_token(prompt, k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    topk_values, topk_indices = torch.topk(logits, top_k, dim=-1)\n",
    "    top_tokens = tokenizer.batch_decode(topk_indices[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    # print(topk_indices[0][:k])\n",
    "    \n",
    "    return top_tokens, topk_values[0]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_token_prop(prompt, k=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    print(tokenizer.batch_decode(\n",
    "        inputs['input_ids'][0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    print(outputs)\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n",
    "\n",
    "    # Apply softmax to top-k logits to get probabilities\n",
    "    topk_probs = F.softmax(topk_values, dim=-1)\n",
    "\n",
    "    top_tokens = tokenizer.batch_decode(\n",
    "        topk_indices[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    return top_tokens, topk_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[4340,  646,  498]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "['How', ' can', ' you']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 8.2367,  6.6111,  4.5391,  ..., -4.0680, -4.0680, -4.0681],\n",
      "         [ 4.1064,  5.5320,  4.0867,  ..., -3.4824, -3.4814, -3.4826],\n",
      "         [ 6.9169,  8.1698,  9.0604,  ..., -3.9949, -3.9945, -3.9951]]]), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7fb1fd963c20>, hidden_states=None, attentions=None)\n",
      "1.  determine (logit: 0.2247)\n",
      "2.  use (logit: 0.1334)\n",
      "3.  find (logit: 0.1147)\n",
      "4.  modify (logit: 0.1048)\n",
      "5.  simplify (logit: 0.0876)\n",
      "6.  calculate (logit: 0.0757)\n",
      "7.  solve (logit: 0.0678)\n",
      "8.  express (logit: 0.0650)\n",
      "9.  prove (logit: 0.0634)\n",
      "10.  create (logit: 0.0630)\n",
      "Sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How can you\"\n",
    "top_k = 10\n",
    "\n",
    "top_tokens, top_values = top_k_token_prop(prompt, k=top_k)\n",
    "for i, (token, score) in enumerate(zip(top_tokens, top_values)):\n",
    "    print(f\"{i+1}. {token} (logit: {score.item():.4f})\")\n",
    "print(f\"Sum: {top_values[0:10].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from arithmetic_coder import ArithmeticDecoder, ArithmeticEncoder, BitInputStream, BitOutputStream\n",
    "\n",
    "\n",
    "def gen_rank(probs, next_token):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True, stable=True)\n",
    "    rank_list = []\n",
    "    if next_token.shape[0] > 1:\n",
    "        for i in range(next_token.shape[0]):\n",
    "            rank_list += [torch.where(probs_idx[i:i+1, :] == next_token[i])[-1]]\n",
    "        rank = torch.squeeze(torch.stack(rank_list))\n",
    "    else:\n",
    "        rank = torch.where(probs_idx == next_token)[-1]\n",
    "    return rank\n",
    "\n",
    "\n",
    "def read_bitstream(bitin):\n",
    "    temp_list = []\n",
    "    while True:\n",
    "        temp = bitin.read()\n",
    "        if temp == -1:\n",
    "            break\n",
    "        temp_list += [temp]\n",
    "    temp_arr = np.array(temp_list)\n",
    "    final_ind = (np.where(temp_arr == 1)[0][-1]).astype(int)\n",
    "    final_arr = temp_arr[:final_ind+1]\n",
    "\n",
    "    return final_arr\n",
    "\n",
    "\n",
    "def print_tokens(tokens):\n",
    "    decoded_tokens = tokenizer.batch_decode(\n",
    "        tokens[0:1000],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    print(\"|\".join(decoded_tokens))\n",
    "\n",
    "\n",
    "def print_probs(probs):\n",
    "    print(\"|\".join([f\"{p:.4f}\" for p in probs]))\n",
    "\n",
    "\n",
    "class LLMzip_encode:\n",
    "    def __init__(self, model, tokenizer, filename, extra_string=None, batch_size=32, win_size=100, max_tokens=10_000):\n",
    "        self.model = model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.filename = filename\n",
    "        self.file_out = open(self.filename+'_llmzip_ac.txt', 'wb')\n",
    "        self.bitout = BitOutputStream(self.file_out)\n",
    "        self.AC_encoder = ArithmeticEncoder(32, self.bitout)\n",
    "\n",
    "        self.AC_encoder = ArithmeticEncoder(32, self.bitout)\n",
    "\n",
    "        self.alphabet_size = self.model.config.vocab_size\n",
    "\n",
    "        self.token_length = 0\n",
    "        self.starter_tokens = []\n",
    "        self.extra_tokens = np.array(tokenizer.encode(extra_string)) if extra_string is not None else None\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.win_size = win_size\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.total_time = 0\n",
    "        self.gpu_time = 0\n",
    "\n",
    "    def encode_batch(self, prompt_tokens):\n",
    "        #print_tokens(prompt_tokens)\n",
    "        bsz = prompt_tokens.shape[0]\n",
    "\n",
    "        prompt_size = prompt_tokens.shape[1]\n",
    "\n",
    "        start = time.time()\n",
    "        tokens = torch.full((bsz, prompt_size), 0).long() # self.tokenizer.pad_token_id (instead of 0)\n",
    "        tokens[:bsz, : prompt_size] = torch.tensor(prompt_tokens).long()\n",
    "        tokens = tokens.to(self.device)\n",
    "\n",
    "        cur_pos = prompt_size-1\n",
    "        prev_pos = 0\n",
    "\n",
    "        logits = self.model.forward(tokens[:, prev_pos:cur_pos]).logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        rank = gen_rank(probs, next_token=tokens[:, cur_pos])\n",
    "\n",
    "        probs_np2 = probs.cpu().detach().numpy()\n",
    "        tokens_np2 = tokens[:, cur_pos].cpu().numpy()\n",
    "        ranks_np2 = rank.cpu().numpy()\n",
    "        self.gpu_time = time.time() - start\n",
    "\n",
    "        probs_tok = probs_np2[np.arange(bsz), tokens_np2]\n",
    "        #print_tokens(tokens_np2)\n",
    "        #print_probs(probs_tok)\n",
    "\n",
    "        cumul = np.zeros(self.model.config.vocab_size+1, dtype=np.uint64)\n",
    "        for j in range(bsz):\n",
    "            prob1 = probs_np2[j]\n",
    "            cumul[1:] = np.cumsum(prob1*10000000 + 1)\n",
    "            self.AC_encoder.write(cumul, tokens_np2[j])\n",
    "\n",
    "        return ranks_np2, probs_tok\n",
    "\n",
    "    def encode(self):\n",
    "        if not os.path.exists(self.filename + '_tokens.npy'):\n",
    "            with open(self.filename, 'r') as f_in:\n",
    "                text_input = f_in.read()\n",
    "\n",
    "            tokens_full = np.array(self.tokenizer.encode(text_input))\n",
    "            print(f\"max token: {np.max(tokens_full)}\")\n",
    "            np.save(self.filename + '_tokens.npy', tokens_full)\n",
    "        else:\n",
    "            tokens_full = np.load(self.filename + '_tokens.npy')\n",
    "\n",
    "        start = time.time()\n",
    "        tokens_full = tokens_full[0:self.max_tokens]\n",
    "        #print_tokens(tokens_full)\n",
    "\n",
    "        win_size_enc = self.win_size + 1  # additional 1 is to pass the true token apart from the context of win_size\n",
    "\n",
    "        ranks_list = []\n",
    "        probs_tok_list = []\n",
    "\n",
    "        n_runs = tokens_full.size-win_size_enc+1\n",
    "\n",
    "        tokens_encoded = tokens_full[self.win_size:self.win_size+n_runs]\n",
    "        #print(self.win_size)\n",
    "        self.starter_tokens = tokens_full[:self.win_size]\n",
    "        np.save(self.filename + '_starter_tokens.npy', self.starter_tokens)\n",
    "\n",
    "        n_batches = np.ceil(n_runs/self.batch_size).astype(int)\n",
    "\n",
    "        for b_ind in range(n_batches):\n",
    "            batch_range_start = b_ind*self.batch_size\n",
    "            batch_range_stop = np.minimum(n_runs, (b_ind+1) * self.batch_size)\n",
    "            # tokens_batch = np.array([np.concatenate(([tokens_full[0]], tokens_full[i:i+win_size_enc])) for i in range(batch_range_start, batch_range_stop)])\n",
    "            if self.extra_tokens is None:\n",
    "                tokens_batch = np.array([tokens_full[i: i + win_size_enc] for i in range(batch_range_start, batch_range_stop)])\n",
    "            else:\n",
    "                tokens_batch = np.array([np.concatenate((self.extra_tokens, tokens_full[i: i + win_size_enc])) for i in range(batch_range_start, batch_range_stop)])\n",
    "            ranks, probs_tok = self.encode_batch(tokens_batch)\n",
    "            ranks_list += [ranks]\n",
    "            probs_tok_list += [probs_tok]\n",
    "\n",
    "            if (b_ind * 100 / n_batches) % 10 == 0:\n",
    "                print(f'Encoder: Completed {int(b_ind * 100 / n_batches)} %')\n",
    "\n",
    "        self.total_time = time.time() - start\n",
    "\n",
    "        ranks_full = np.concatenate(ranks_list, 0).squeeze()\n",
    "        probs_tok_full = np.concatenate(probs_tok_list, 0).squeeze()\n",
    "\n",
    "        self.token_length = len(tokens_encoded)\n",
    "\n",
    "        self.AC_encoder.finish()\n",
    "        self.bitout.close()\n",
    "        self.file_out.close()\n",
    "\n",
    "        self.compute_compression_ratio(tokens_encoded, probs_tok_full)\n",
    "\n",
    "    def decode(self):\n",
    "        # Open the compressed bit-stream\n",
    "        with open(self.filename + '_llmzip_ac.txt', 'rb') as f_in:\n",
    "            bitin   = BitInputStream(f_in)\n",
    "            decoder = ArithmeticDecoder(32, bitin)\n",
    "\n",
    "            # Load initial window\n",
    "            decoded = list(self.starter_tokens[:self.win_size])\n",
    "            n_to_decode = self.token_length\n",
    "\n",
    "            # Loop through tokens\n",
    "            for _ in range(n_to_decode):\n",
    "                # build context from win_size tokens\n",
    "                ctx = torch.tensor([decoded[-self.win_size:]])\n",
    "                ctx = ctx.to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(ctx).logits[:, -1, :]\n",
    "                    probs  = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "                # rebuild distribution for the next token\n",
    "                cumul = np.zeros(probs.shape[0] + 1, dtype=np.uint64)\n",
    "                cumul[1:] = np.cumsum(probs * 10_000_000 + 1)\n",
    "\n",
    "                # pull one symbol out of the bit-stream\n",
    "                sym = decoder.read(cumul, alphabet_size=self.alphabet_size)\n",
    "                decoded.append(int(sym))\n",
    "\n",
    "        # convert to text\n",
    "        return self.tokenizer.decode(decoded)\n",
    "\n",
    "    def compute_compression_ratio(self, tokens_encoded, probs_tok):\n",
    "        text_encoded = self.tokenizer.decode(tokens_encoded.squeeze().tolist())\n",
    "\n",
    "        N_T = tokens_encoded.size\n",
    "        N_C = len(text_encoded)\n",
    "\n",
    "        df_out = {}\n",
    "        df_out['characters'] = N_C\n",
    "        df_out['tokens'] = N_T\n",
    "\n",
    "        entropy_val = np.sum(-np.log2(probs_tok)) / N_C\n",
    "        df_out['entropy'] = [f\"{entropy_val:.4f}\"]\n",
    "\n",
    "        file_in = open(self.filename+\"_llmzip_ac.txt\", 'rb')\n",
    "        bitin = BitInputStream(file_in)\n",
    "        compressed_bits = read_bitstream(bitin)\n",
    "        rho_AC = compressed_bits.size/N_C\n",
    "        print(f'Compression Ratio for Arithmetic Coding :  {rho_AC} bits/char')\n",
    "        file_in.close()\n",
    "\n",
    "        df_out['Llama+AC compressed file size'] = compressed_bits.size\n",
    "        df_out['bits per character'] = rho_AC\n",
    "        df_out['throughput (bits/s)'] = N_C / self.total_time\n",
    "        print(f'Throughput: {int(N_C / self.total_time)} bytes/s')\n",
    "        print(f'total time: {self.total_time}, gpu time: {self.gpu_time}')\n",
    "\n",
    "        print(df_out)\n",
    "\n",
    "        with open(self.filename+'_metrics.json', 'w') as file_metrics:\n",
    "            json.dump(df_out, file_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Encoder = LLMzip_encode(model, tokenizer, filename='../test.txt', win_size=13)\n",
    "Encoder.encode()\n",
    "#recovered_text = Encoder.decode()\n",
    "#print(\"Recovered text:\", recovered_text)\n",
    "\n",
    "Encoder = LLMzip_encode(model, tokenizer, filename='../test.txt', extra_string='bank, ', win_size=13)\n",
    "Encoder.encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Completed 0 %\n",
      "Encoder: Completed 10 %\n",
      "Encoder: Completed 20 %\n",
      "Encoder: Completed 30 %\n",
      "Encoder: Completed 40 %\n",
      "Encoder: Completed 50 %\n",
      "Encoder: Completed 60 %\n",
      "Encoder: Completed 70 %\n",
      "Encoder: Completed 80 %\n",
      "Encoder: Completed 90 %\n",
      "Compression Ratio for Arithmetic Coding :  0.9711058908565233 bits/char\n",
      "Throughput: 1444 bytes/s\n",
      "total time: 38.31452965736389, gpu time: 0.04167461395263672\n",
      "{'characters': 55340, 'tokens': 9900, 'entropy': ['0.9681'], 'Llama+AC compressed file size': 53741, 'bits per character': 0.9711058908565233, 'throughput (bits/s)': 1444.3606771345003}\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model, tokenizer, filename='../text8.txt', win_size=100)\n",
    "Encoder.encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Completed 0 %\n",
      "Encoder: Completed 10 %\n",
      "Encoder: Completed 20 %\n",
      "Encoder: Completed 30 %\n",
      "Encoder: Completed 40 %\n",
      "Encoder: Completed 50 %\n",
      "Encoder: Completed 60 %\n",
      "Encoder: Completed 70 %\n",
      "Encoder: Completed 80 %\n",
      "Encoder: Completed 90 %\n",
      "Compression Ratio for Arithmetic Coding :  0.9043212328223051 bits/char\n",
      "Throughput: 169 bytes/s\n",
      "total time: 297.659095287323, gpu time: 0.12717437744140625\n",
      "{'characters': 50356, 'tokens': 9000, 'entropy': ['0.9012'], 'Llama+AC compressed file size': 45538, 'bits per character': 0.9043212328223051, 'throughput (bits/s)': 169.1733959998521}\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model, tokenizer, filename='../text8.txt', batch_size=4, win_size=1000)\n",
    "Encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Completed 0 %\n",
      "Compression Ratio for Arithmetic Coding :  1.3040557486295725 bits/char\n",
      "Throughput: 3962 bytes/s\n",
      "total time: 14.08653974533081, gpu time: 0.02246546745300293\n",
      "{'characters': 55822, 'tokens': 9990, 'entropy': ['1.3045'], 'Llama+AC compressed file size': 72795, 'bits per character': 1.3040557486295725, 'throughput (bits/s)': 3962.7900825327256}\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model, tokenizer, filename='../text8.txt', batch_size=64, win_size=10)\n",
    "Encoder.encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "model2 = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer2 = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LLMzip_encode(model3, tokenizer3, filename='../text8_3.txt', batch_size=1, win_size=10, max_tokens=100_000)\n",
    "Encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Completed 0 %\n",
      "Compression Ratio for Arithmetic Coding :  1.3245215160296426 bits/char\n",
      "Throughput: 5692 bytes/s\n",
      "total time: 92.03052592277527, gpu time: 0.05153179168701172\n",
      "{'characters': 523842, 'tokens': 99900, 'entropy': ['1.3243'], 'Llama+AC compressed file size': 693840, 'bits per character': 1.3245215160296426, 'throughput (bits/s)': 5692.046141728742}\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model3, tokenizer3, filename='../text8_3.txt', batch_size=256, win_size=100, max_tokens=100_000)\n",
    "\n",
    "Encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Completed 0 %\n",
      "Encoder: Completed 50 %\n",
      "Compression Ratio for Arithmetic Coding :  1.5966639457442995 bits/char\n",
      "Throughput: 13922 bytes/s\n",
      "total time: 37.660134077072144, gpu time: 0.00895071029663086\n",
      "{'characters': 524332, 'tokens': 99990, 'entropy': ['1.5985'], 'Llama+AC compressed file size': 837182, 'bits per character': 1.5966639457442995, 'throughput (bits/s)': 13922.733225722062}\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model3, tokenizer3, filename='../text8_3.txt', batch_size=128, win_size=10, max_tokens=100_000)\n",
    "\n",
    "Encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Completed 0 %\n",
      "Compression Ratio for Arithmetic Coding :  1.5966868320072016 bits/char\n",
      "Throughput: 12032 bytes/s\n",
      "total time: 43.57472252845764, gpu time: 0.3364715576171875\n",
      "{'characters': 524332, 'tokens': 99990, 'entropy': ['1.5985'], 'Llama+AC compressed file size': 837194, 'bits per character': 1.5966868320072016, 'throughput (bits/s)': 12032.939501968622}\n"
     ]
    }
   ],
   "source": [
    "Encoder = LLMzip_encode(model3, tokenizer3, filename='../text8_3.txt', batch_size=2048, win_size=10, max_tokens=100_000)\n",
    "\n",
    "Encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current index: 0\n",
      "Prompt (0): \n",
      "stored_data (0): []\n",
      "\n",
      "\n",
      "Current index: 1\n",
      "Prompt (1): A\n",
      "stored_data (1): ['A']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m     i += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     top_tokens, top_values = \u001b[43mtop_k_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j, (token, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(top_tokens, top_values)):\n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (logit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtop_k_token\u001b[39m\u001b[34m(prompt, k)\u001b[39m\n\u001b[32m      2\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m logits = outputs.logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m      7\u001b[39m topk_values, topk_indices = torch.topk(logits, top_k, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m output_hidden_states = (\n\u001b[32m    699\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    700\u001b[39m )\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:405\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `past_key_values` should be either a `Cache` object or `None`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     past_key_values = DynamicCache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/summer-offsite/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# string_data = \"1. \"\n",
    "# string_data = \"\"\n",
    "string_data = \"Alice was beginning to get very tired of sitting by her sister on the bank\"\n",
    "# string_data = \"En 1815, M. Charles-Franois-Bienvenu Myriel tait vque de Digne. C'tait un vieillard d'environ soixante-quinze ans; il occupait le sige de Digne depuis 1806.\"\n",
    "top_k = 10\n",
    "\n",
    "stored_data = []\n",
    "prompt = \"\"\n",
    "i = 0\n",
    "while i < len(string_data):\n",
    "    print(f\"\\n\\nCurrent index: {i}\")\n",
    "    print(f\"Prompt ({i}): {prompt}\")\n",
    "    print(f\"stored_data ({i}): {stored_data}\")\n",
    "    if prompt == \"\":\n",
    "        prompt = string_data[i]\n",
    "        stored_data.append(string_data[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        top_tokens, top_values = top_k_token(prompt, k=top_k)\n",
    "        for j, (token, score) in enumerate(zip(top_tokens, top_values)):\n",
    "            print(f\"{j+1}. '{token}' (logit: {score.item():.4f})\")\n",
    "            token_len = len(token)\n",
    "            print(f\"\\tToken length: {token_len}\")\n",
    "            print(f\"\\tComparing with string_data[{i}:{i+token_len}] = '{string_data[i:i+token_len]}'\")\n",
    "            if token == string_data[i:i+token_len]:\n",
    "                stored_data.append(j)\n",
    "                prompt += token\n",
    "                i += token_len - 1\n",
    "                break\n",
    "            elif j == top_k - 1:\n",
    "                print(f\"Character '{string_data[i]}' not found in top {top_k} tokens.\")\n",
    "                stored_data.append(string_data[i])\n",
    "                prompt += string_data[i]\n",
    "        i += 1\n",
    "        # if i == 6:\n",
    "        #     break\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal store_data: {stored_data}\")\n",
    "print(f\"original string_data: {string_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
